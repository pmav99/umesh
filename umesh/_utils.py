# pyright: reportUnknownMemberType=false
# pyright: reportUnknownVariableType=false
from __future__ import annotations

import collections
import io
import itertools
import os
import typing as T

import numexpr
import numpy as np


def _readline(fd: io.BufferedReader) -> bytes:
    return fd.readline().split(b"=")[0].split(b"!")[0].strip()


def split_node_group(nodes, group_size=3):
    previous = None
    for group in itertools.batched(nodes, group_size):
        current = list(group)
        if previous:
            yield previous[-1:] + current
        else:
            yield current
        previous = current


def write_gr3(
    filepath: os.PathLike[str] | str,
    nodes,
    elements,
    boundaries: dict[str, list[int]] | None = None,
    comment: str = "Generated by pyPoseidon",
) -> None:
    nodes_index = np.arange(1, len(nodes) + 1).reshape(1, -1).T
    elements_index = np.arange(1, len(elements) + 1).reshape(1, -1).T
    with open(filepath, "wb") as fd:
        fd.write(f"{comment}\n".encode("utf-8"))
        (fd.write(f"{len(elements)} {len(nodes)}\n".encode("utf-8")),)
        np.savetxt(
            fd,
            np.concatenate((nodes_index, nodes), axis=1),
            fmt="%d %f %f %f",
        )
        np.savetxt(
            fd,
            np.concatenate(
                (elements_index, np.full_like(elements_index, elements.shape[1]), elements + 1),
                axis=1,
            ),
            fmt="%d",
        )
        if boundaries:
            # open
            open_boundaries = boundaries.get(-1, {})
            total_open_boundaries_nodes = 0
            open_lines = []
            for index, nodes in enumerate(open_boundaries, 1):
                no_nodes = len(nodes)
                total_open_boundaries_nodes += no_nodes
                open_lines.append(f"{no_nodes} = Number of nodes for open boundary {index}")
                open_lines.append("\n".join((np.asarray(nodes) + 1).astype(str)))
            lines = [
                f"{len(open_boundaries)} = Number of open boundaries",
                f"{total_open_boundaries_nodes} = Total number of open boundary nodes",
            ] + open_lines
            fd.write(("\n".join(lines) + "\n").encode("utf-8"))
            # closed
            total_closed_boundaries = 0
            total_closed_boundaries_nodes = 0
            closed_lines = []
            for boundary_type, boundary_groups in boundaries.items():
                if boundary_type == -1:
                    continue
                index = 1
                for nodes in boundary_groups:
                    # total_closed_boundaries_nodes += len(nodes)
                    for i, split in enumerate(split_node_group(nodes, 1000)):
                        total_closed_boundaries += 1
                        total_closed_boundaries_nodes += len(split)
                        closed_lines.append(
                            f"{len(split)} {boundary_type} = Number of nodes for land boundary {index}",
                        )
                        closed_lines.append("\n".join((np.asarray(split) + 1).astype(str)))
                        index += 1
            lines = [
                f"{total_closed_boundaries} = Number of land boundaries",
                f"{total_closed_boundaries_nodes} = Total number of land boundary nodes",
            ] + closed_lines
            fd.write("\n".join(lines).encode("utf-8"))


# To test this:
#    for path in sorted(pathlib.Path("/path/to/schism_verification_tests/").glob("**/hgrid.gr3")):
#         path
#         _ = parse_gr3(path)
def parse_gr3(
    path: os.PathLike[str] | str,
    include_boundaries: bool = False,
    sep: str | None = None,
) -> dict[str, T.Any]:
    """
    Parse an hgrid.gr3 file.

    The function is also able to handle fort.14 files, too, (i.e. ADCIRC)
    but the boundary parsing is not keeping all the available information.
    """
    rvalue: dict[str, T.Any] = {}
    with open(path, "rb") as fd:
        comment = fd.readline()  # skip line
        no_elements, no_points = map(int, fd.readline().strip().split(b"!")[0].split())
        nodes_buffer = io.BytesIO(b"\n".join(itertools.islice(fd, 0, no_points)))
        nodes = np.loadtxt(nodes_buffer, delimiter=sep, usecols=(1, 2, 3))
        elements_buffer = io.BytesIO(b"\n".join(itertools.islice(fd, 0, no_elements)))
        elements = np.loadtxt(elements_buffer, delimiter=sep, usecols=(2, 3, 4), dtype=int)
        elements -= 1  # 0-based index for the nodes
        rvalue["comment"] = comment
        rvalue["nodes"] = nodes
        rvalue["elements"] = elements
        # boundaries
        if include_boundaries:
            # boundaries = collections.defaultdict(lambda: collections.defaultdict(list))
            boundaries = collections.defaultdict(list)
            no_open_boundaries = int(_readline(fd))
            # total_open_boundary_nodes = int(_readline(fd))
            _ = int(_readline(fd))
            for i in range(no_open_boundaries):
                no_nodes_in_boundary = int(_readline(fd))
                boundary_nodes = np.genfromtxt(
                    fd,
                    delimiter=sep,
                    usecols=(0,),
                    max_rows=no_nodes_in_boundary,
                    dtype=int,
                )
                boundaries[-1].append(boundary_nodes - 1)  # 0-based index
            # closed boundaries
            no_closed_boundaries = int(_readline(fd))
            # total_closed_boundary_nodes = int(_readline(fd))
            _ = int(_readline(fd))
            for i in range(no_closed_boundaries):
                # Sometimes it seems that the closed boundaries don't have a "type indicator"
                # For example: Test_COSINE_SFBay/hgrid.gr3
                # In this cases we assume that boundary type is 0 (i.e. land in schism)
                # XXX Maybe check the source code?
                parsed = _readline(fd).split(b" ")
                if len(parsed) == 1:
                    no_nodes_in_boundary = int(parsed[0])
                    boundary_type = 0
                else:
                    no_nodes_in_boundary, boundary_type = map(int, (p for p in parsed if p))
                boundary_nodes = np.genfromtxt(
                    fd,
                    delimiter=sep,
                    usecols=(0,),
                    max_rows=no_nodes_in_boundary,
                    dtype=int,
                )
                boundaries[boundary_type].append(boundary_nodes - 1)  # 0-based index
            rvalue["boundaries"] = boundaries
    return rvalue


def is_ccw(nodes, triangles):
    """
    Return a boolean array indicating if each triangle in 'triangles' is CCW

    Parameters:
    - nodes: Array of shape (N, 2) containing (x, y) coordinates of vertices.
    - triangles: Array of shape (M, 3) containing vertex indices for each triangle.

    Returns:
    - is_ccw: Boolean array of shape (M,) where True indicates CCW, False indicates CW.
    """
    # Extract the three vertices of each triangle: A, B, C
    A, B, C = nodes[:, :2][triangles.T]

    # Compute vectors AB and AC
    AB_x, AB_y = (B - A).T
    AC_x, AC_y = (C - A).T

    signed_areas = numexpr.evaluate("(AB_x * AC_y) - (AB_y * AC_x)")

    # if CCW then signed area > 0
    return signed_areas > 0


def ensure_triangles_are_ccw(nodes, triangles):
    is_clockwise = ~is_ccw(nodes, triangles)
    if is_clockwise.any():
        triangles[is_clockwise] = triangles[is_clockwise][:, [0, 2, 1]]


def get_boundary_edges(triangles):
    # We want to create a unique ID per edge.
    # To do this we create an array of all the edges and we sort it so that the
    # first node is a smaller number than the second node
    # edges = triangles[:, [0,1,1,2,2,0]].reshape(-1, 2)
    edges = np.sort(
        np.concatenate(
            (
                triangles[:, :2],
                triangles[:, 1:],
                triangles[:, ::2],
            ),
            axis=0,
        ),
    )

    # Now that we have the sorted edges we can use Cantor pairing
    # to create the unique IDs
    k1, k2 = edges[:, 0], edges[:, 1]  # noqa: F841
    edge_ids = numexpr.evaluate("(k1 + k2) * (k1 + k2 + 1) / 2 + k2").astype(np.int64)

    # Extract unique edges
    edge_ids, indices, counts = np.unique(
        edge_ids,
        return_index=True,
        return_counts=True,
    )
    indices = indices[counts == 1]
    return edges[indices]


def is_crossing_antimeridian(
    nodes,
    triangles,
    max_lon: float = 10,
):
    X = nodes[:, 0]
    lon_a, lon_b, lon_c = X[triangles].T
    condition = (
        # fmt: off
        ((lon_a * lon_b < 0) & (np.abs(lon_a - lon_b) > max_lon))
        | ((lon_a * lon_c < 0) & (np.abs(lon_a - lon_c) > max_lon))
        | ((lon_b * lon_c < 0) & (np.abs(lon_b - lon_c) > max_lon))
        # fmt: on
    )
    return condition
    # `np.asarray(condition).nonzero()` is equivalent to `np.where(condition)` but faster
    not_crossing_antimeridian = np.asarray(~condition).nonzero()[0]
    return not_crossing_antimeridian
